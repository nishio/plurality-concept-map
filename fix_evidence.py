#!/usr/bin/env python3
"""
è¨¼æ‹ ãƒ†ã‚­ã‚¹ãƒˆã®ä¸æ­£ç¢ºãªå¼•ç”¨ã‚’åŸæ–‡ã«åŸºã¥ã„ã¦è‡ªå‹•ä¿®æ­£ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from difflib import SequenceMatcher


def load_graph_data(graph_file: Path) -> Dict:
    """ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    with open(graph_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_graph_data(graph_file: Path, data: Dict):
    """ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    with open(graph_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_source_text(source_file: Path) -> str:
    """ã‚½ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    with open(source_file, 'r', encoding='utf-8') as f:
        content = f.read()
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€å†’é ­ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»
        lines = content.split('\n')
        # URLã‚„ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æœ¬æ–‡ã‚’å–å¾—
        content_start = 0
        for i, line in enumerate(lines):
            if line.strip() and not line.startswith('http') and '|' not in line and not line.startswith('#'):
                # æœ¬æ–‡ã®é–‹å§‹ã‚’æ¤œå‡º
                content_start = i
                break
        return '\n'.join(lines[content_start:])


def normalize_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ï¼ˆç©ºç™½ã€æ”¹è¡Œã€å¥èª­ç‚¹ã®çµ±ä¸€ï¼‰"""
    # æ”¹è¡Œã‚’ç©ºç™½ã«å¤‰æ›
    text = re.sub(r'\s+', ' ', text)
    # å¥èª­ç‚¹å‰å¾Œã®ç©ºç™½ã‚’èª¿æ•´
    text = re.sub(r'\s*([ã€‚ã€ï¼ï¼Ÿ])\s*', r'\1', text)
    # å…¨è§’ãƒ»åŠè§’ã®çµ±ä¸€
    text = text.replace('ã€€', ' ')  # å…¨è§’ç©ºç™½ã‚’åŠè§’ã«
    return text.strip()


def find_best_match_in_source(evidence_text: str, source_text: str) -> Optional[str]:
    """
    ã‚½ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆå†…ã§è¨¼æ‹ ãƒ†ã‚­ã‚¹ãƒˆã«æœ€ã‚‚è¿‘ã„éƒ¨åˆ†ã‚’æ¤œç´¢ã—ã€æ­£ç¢ºãªåŸæ–‡ã‚’è¿”ã™
    """
    evidence_normalized = normalize_text(evidence_text)
    source_normalized = normalize_text(source_text)
    
    # ã¾ãšå®Œå…¨ä¸€è‡´ã‚’è©¦è¡Œ
    if evidence_normalized in source_normalized:
        return evidence_text  # æ—¢ã«æ­£ç¢º
    
    # éƒ¨åˆ†ä¸€è‡´ã‚’æ¤œç´¢
    evidence_words = evidence_normalized.split()
    if len(evidence_words) < 3:
        return None  # çŸ­ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    
    best_match = None
    best_similarity = 0.7  # æœ€ä½70%ã®é¡ä¼¼åº¦ã‚’è¦æ±‚
    
    # å‰å¾Œã«ä½™è£•ã‚’æŒãŸã›ãŸæ¤œç´¢ç¯„å›²
    for window_factor in [1.0, 1.2, 0.8]:  # åŒã˜é•·ã•ã€20%é•·ã„ã€20%çŸ­ã„
        window_size = int(len(evidence_normalized) * window_factor)
        
        for i in range(len(source_normalized) - window_size + 1):
            window_text = source_normalized[i:i + window_size]
            similarity = SequenceMatcher(None, evidence_normalized, window_text).ratio()
            
            if similarity > best_similarity:
                best_similarity = similarity
                # æ­£è¦åŒ–å‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¯¾å¿œã™ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º
                # ç°¡å˜ãªæ–¹æ³•ï¼šsource_textã§åŒæ§˜ã®ä½ç½®ã‚’æ¢ã™
                best_match = extract_original_text(source_text, window_text, evidence_text)
    
    return best_match


def extract_original_text(source_text: str, normalized_match: str, original_evidence: str) -> str:
    """
    æ­£è¦åŒ–ã•ã‚ŒãŸãƒãƒƒãƒãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å…ƒã®ã‚½ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã§ã®å¯¾å¿œéƒ¨åˆ†ã‚’æŠ½å‡º
    """
    # æ­£è¦åŒ–ã•ã‚ŒãŸãƒãƒƒãƒãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã¨æœ€å¾Œã®æ•°èªã‚’ä½¿ã£ã¦ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢
    match_words = normalized_match.split()
    if len(match_words) < 3:
        return normalized_match
    
    # æœ€åˆã®3èªã¨æœ€å¾Œã®3èªã§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
    start_pattern = ' '.join(match_words[:3])
    end_pattern = ' '.join(match_words[-3:])
    
    # ã‚½ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¯¾å¿œéƒ¨åˆ†ã‚’æ¤œç´¢
    source_normalized = normalize_text(source_text)
    start_pos = source_normalized.find(start_pattern)
    end_pos = source_normalized.rfind(end_pattern)
    
    if start_pos != -1 and end_pos != -1 and start_pos <= end_pos:
        # æ­£è¦åŒ–ã•ã‚Œã¦ã„ãªã„ã‚½ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¯¾å¿œéƒ¨åˆ†ã‚’å¤§ã¾ã‹ã«æŠ½å‡º
        # ã‚ˆã‚Šæ­£ç¢ºãªå®Ÿè£…ãŒå¿…è¦ã ãŒã€ã¨ã‚Šã‚ãˆãšæ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
        extracted = source_normalized[start_pos:end_pos + len(end_pattern)]
        return extracted.strip()
    
    return normalized_match


def fix_evidence_in_graph(graph_file: Path, source_dir: Path) -> int:
    """ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®è¨¼æ‹ ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿®æ­£"""
    graph_data = load_graph_data(graph_file)
    
    # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
    if 'extra-1' in str(graph_file):
        source_file = source_dir / '1.md'
    elif 'extra-2' in str(graph_file):
        source_file = source_dir / '2.md'
    elif 'extra-3' in str(graph_file):
        source_file = source_dir / '3.md'
    else:
        print(f"Unknown graph file pattern: {graph_file}")
        return 0
    
    if not source_file.exists():
        print(f"âŒ Source file not found: {source_file}")
        return 0
    
    source_text = load_source_text(source_file)
    fixes_made = 0
    
    # ç‰¹åˆ¥ãªä¿®æ­£ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ‰‹å‹•ã§ç¢ºèªã—ãŸæ­£ç¢ºãªå¼•ç”¨ï¼‰
    manual_fixes = {
        # extra-1ã®ä¿®æ­£
        "ç§ãŸã¡ã“ããŒæœªæ¥ã®å…±åŒè¨­è¨ˆè€…ã§ã‚ã‚‹ã€‚": "ç§ãŸã¡ã“ããŒæœªæ¥ã®å…±åŒè¨­è¨ˆè€…ã§ã‚ã‚‹",
        "æ–°ãŸãªå¯¾è©±ã‚’å§‹ã‚ã¦æºã‚’åŸ‹ã‚ã€å‘¨ç¸åŒ–ã•ã‚ŒãŒã¡ãªå£°ã‚’å¢—å¹…ã™ã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’è¨­è¨ˆã—ãŸã‚Šã€‚": "æ–°ãŸãªå¯¾è©±ã‚’å§‹ã‚ã¦æºã‚’åŸ‹ã‚ã€å‘¨ç¸åŒ–ã•ã‚ŒãŒã¡ãªå£°ã‚’å¢—å¹…ã™ã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’è¨­è¨ˆã—ãŸã‚Š",
        "ãƒ—ãƒ«ãƒ©ãƒªãƒ†ã‚£ã¯ã€Œç¤¾ä¼šçš„å·®ç•°ã‚’è¶…ãˆãŸã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®æŠ€è¡“ã€ã¨å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚": "ã€Œç¤¾ä¼šçš„å·®ç•°ã‚’è¶…ãˆãŸã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®æŠ€è¡“ã€ã¨å®šç¾©ã•ã‚Œã¦ã„ã¾ã™",
        "ä¸­å¤®é›†æ¨©çš„ãªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãŒã€ç§ãŸã¡ã®ã¤ãªãŒã‚Šã‹ã‚‰ä¾¡å€¤ã‚’æ¾å–ã™ã‚‹ä¸€æ–¹ã§ã€ç¤¾ä¼šçš„ãªçµã³ã¤ãã‚’æ ¹ã“ããå¥ªã†æã‚ŒãŒã‚ã‚‹ã€‚": "ä¸­å¤®é›†æ¨©çš„ãªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãŒã€ç§ãŸã¡ã®ã¤ãªãŒã‚Šã‹ã‚‰ä¾¡å€¤ã‚’æ¾å–ã™ã‚‹ä¸€æ–¹ã§ã€å…±æœ‰ã—ã¦ã„ã‚‹ã¨ã„ã†ç¾å®Ÿæ„Ÿã‚’æãªã‚ã›ã€ç¤¾ä¼šçš„ãªçµã³ã¤ãã‚’æ ¹ã“ããå¥ªã†æã‚ŒãŒã‚ã‚‹",
        
        # extra-2ã®ä¿®æ­£  
        "ã“ã‚Œã»ã©ã®æ–‡åŒ–çš„å·®ç•°ã‚„ã€ã“ã®å¯¾è©±ã«åˆ°ã‚‹ã¾ã§ã®ã¾ã£ãŸãç•°ãªã‚‹é“ç­‹ã‚’è¶…ãˆã¦ç§ãŸã¡ã®è¦–ç‚¹ãŒå…±æŒ¯ã—ãŸã€‚": "ã“ã‚Œã»ã©ã®æ–‡åŒ–çš„å·®ç•°ã‚„ã€ã“ã®å¯¾è©±ã«åˆ°ã‚‹ã¾ã§ã®ã¾ã£ãŸãç•°ãªã‚‹é“ç­‹ã‚’è¶…ãˆã¦ç§ãŸã¡ã®è¦–ç‚¹ãŒå…±æŒ¯ã—ãŸã“ã¨ã§ã€ä¼šã®çµ‚ã‚ã‚Šé ƒã«ãªã‚‹ã¨ç§ã¯ç›®é ­ãŒç†±ããªã£ãŸã»ã©ã ã£ãŸ",
        "ãƒ—ãƒ«ãƒ©ãƒªãƒ†ã‚£ã¯ã€å°æ¹¾ã®ãƒ‡ã‚¸ã‚¿ãƒ«æ°‘ä¸»ä¸»ç¾©ã‚’ç‰½å¼•ã™ã‚‹åˆä»£ãƒ‡ã‚¸ã‚¿ãƒ«å¤§è‡£ã‚ªãƒ¼ãƒ‰ãƒªãƒ¼ãƒ»ã‚¿ãƒ³ã¨ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆé¦–å¸­ç ”ç©¶å“¡ã«ã—ã¦æ°—é‹­ã®çµŒæ¸ˆå­¦è€…Eãƒ»ã‚°ãƒ¬ãƒ³ãƒ»ãƒ¯ã‚¤ãƒ«ãŒæå”±ã™ã‚‹æ–°ãŸãªç¤¾ä¼šãƒ“ã‚¸ãƒ§ãƒ³ã§ã™ã€‚": "ãƒ—ãƒ«ãƒ©ãƒªãƒ†ã‚£ï¼ˆå¤šå…ƒæ€§ï¼‰ã¯ã€å°æ¹¾ã®ãƒ‡ã‚¸ã‚¿ãƒ«æ°‘ä¸»ä¸»ç¾©ã‚’ç‰½å¼•ã™ã‚‹åˆä»£ãƒ‡ã‚¸ã‚¿ãƒ«å¤§è‡£ã‚ªãƒ¼ãƒ‰ãƒªãƒ¼ãƒ»ã‚¿ãƒ³ã¨ãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆé¦–å¸­ç ”ç©¶å“¡ã«ã—ã¦æ°—é‹­ã®çµŒæ¸ˆå­¦è€…Eãƒ»ã‚°ãƒ¬ãƒ³ãƒ»ãƒ¯ã‚¤ãƒ«ãŒæå”±ã™ã‚‹æ–°ãŸãªç¤¾ä¼šãƒ“ã‚¸ãƒ§ãƒ³ã§ã™",
        "æ–‡åŒ–çš„å·®ç•°ã‚„ã€ã“ã®å¯¾è©±ã«åˆ°ã‚‹ã¾ã§ã®ã¾ã£ãŸãç•°ãªã‚‹é“ç­‹ã‚’è¶…ãˆã¦ç§ãŸã¡ã®è¦–ç‚¹ãŒå…±æŒ¯ã—ãŸã€‚": "æ–‡åŒ–çš„å·®ç•°ã‚„ã€ã“ã®å¯¾è©±ã«åˆ°ã‚‹ã¾ã§ã®ã¾ã£ãŸãç•°ãªã‚‹é“ç­‹ã‚’è¶…ãˆã¦ç§ãŸã¡ã®è¦–ç‚¹ãŒå…±æŒ¯ã—ãŸã“ã¨ã§ã€ä¼šã®çµ‚ã‚ã‚Šé ƒã«ãªã‚‹ã¨ç§ã¯ç›®é ­ãŒç†±ããªã£ãŸã»ã©ã ã£ãŸ",
        "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¼çµ±ã®ã€å¾…ã¡ã«å¾…ã£ãŸã€ãã—ã¦å¿…ç„¶çš„ãªãƒªãƒã‚¤ãƒãƒ«ã¨ã—ã¦è¦‹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚": "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¼çµ±ã®ã€å¾…ã¡ã«å¾…ã£ãŸã€ãã—ã¦å¿…ç„¶çš„ãªãƒªãƒã‚¤ãƒãƒ«ã¨ã—ã¦è¦‹ã‚‰ã‚Œã¦ã„ã‚‹",
        "ç¤¾ä¼šçš„å·®ç•°ã‚’è¶…ãˆãŸã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®æŠ€è¡“ã¨å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚": "ã€Œç¤¾ä¼šçš„å·®ç•°ã‚’è¶…ãˆãŸã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®æŠ€è¡“ã€ã¨å®šç¾©ã•ã‚Œã¦ã„ã¾ã™",
        
        # extra-3ã®ä¿®æ­£
        "ã€ã‚³ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ã«ã¯ã“ã®æ„å‘³åˆã„ã‚‚å«ã¾ã‚Œã‚‹ãŒã€å¿…ãšã—ã‚‚å‰µé€ çš„ãªç›¸ä¹—åŠ¹æœã‚’ä¼´ã‚ãšã€ã‚ˆã‚Šå–å¼•çš„ãªç›¸äº’ä½œç”¨ã‚’æŒ‡ã™å ´åˆã‚‚ã‚ã‚‹ã€‚": "ã€ã‚³ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ã«ã¯ã“ã®æ„å‘³åˆã„ã‚‚å«ã¾ã‚Œã‚‹ãŒã€å¿…ãšã—ã‚‚å‰µé€ çš„ãªç›¸ä¹—åŠ¹æœã‚’ä¼´ã‚ãšã€ã‚ˆã‚Šå–å¼•çš„ãªç›¸äº’ä½œç”¨ã‚’æŒ‡ã™å ´åˆã‚‚ã‚ã‚‹",
        "QVã§ã¯ã€å„å‚åŠ è€…ã«ä¸ãˆã‚‰ã‚ŒãŸã€ŒæŠ•ç¥¨æ¨©ï¼ˆãƒœã‚¤ã‚¹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆï¼‰ã€ã‚’è‡ªç”±ã«é…åˆ†ã—ã¦è¤‡æ•°ç¥¨ã‚’æŠ•ã˜ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚": "QVã§ã¯ã€å„å‚åŠ è€…ã«ä¸ãˆã‚‰ã‚ŒãŸã€ŒæŠ•ç¥¨æ¨©ï¼ˆãƒœã‚¤ã‚¹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆï¼‰ã€ã‚’è‡ªç”±ã«é…åˆ†ã—ã¦è¤‡æ•°ç¥¨ã‚’æŠ•ã˜ã‚‹ã“ã¨ãŒã§ãã‚‹",
        "AIãŒå¤§è¦æ¨¡ãªã‚³ãƒ¡ãƒ³ãƒˆã®åˆ†é¡ã‚„è¦ç´„ä½œæ¥­ã‚’ä»£è¡Œã—ãŸã‚‰ã©ã†ãªã‚‹ã‹ã€‚": "AIãŒå¤§è¦æ¨¡ãªã‚³ãƒ¡ãƒ³ãƒˆã®åˆ†é¡ã‚„è¦ç´„ä½œæ¥­ã‚’ä»£è¡Œã—ãŸã‚‰ã©ã†ãªã‚‹ã‹",
        
        # â¿»è¨˜å·ã®å•é¡Œ
        "ã“ã®åšç‰©é¤¨ãŒç§ã®æƒ³åƒã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«â¿»ã®ç²¾ç¥ã‚’ãƒ›ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ä½“ç¾ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã£ã¦ããŸã€‚": "ã“ã®åšç‰©é¤¨ãŒç§ã®æƒ³åƒã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«ãƒ—ãƒ«ãƒ©ãƒªãƒ†ã‚£ã®ç²¾ç¥ã‚’ãƒ›ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ä½“ç¾ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã£ã¦ããŸ",
        "ã“ã®åšç‰©é¤¨ãŒç§ã®æƒ³åƒã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«ãƒ—ãƒ«ãƒ©ãƒªãƒ†ã‚£ã®ç²¾ç¥ã‚’ãƒ›ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ä½“ç¾ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã£ã¦ããŸã€‚": "ã“ã®åšç‰©é¤¨ãŒç§ã®æƒ³åƒã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«ãƒ—ãƒ«ãƒ©ãƒªãƒ†ã‚£ã®ç²¾ç¥ã‚’ãƒ›ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ä½“ç¾ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã£ã¦ããŸ"
    }
    
    # ãƒãƒ¼ãƒ‰ã®è¨¼æ‹ ã‚’ä¿®æ­£
    for node in graph_data.get('nodes', []):
        evidences = node.get('evidence', [])
        for i, evidence_item in enumerate(evidences):
            if isinstance(evidence_item, dict):
                evidence_text = evidence_item.get('text', '')
                if evidence_text in manual_fixes:
                    evidence_item['text'] = manual_fixes[evidence_text]
                    fixes_made += 1
                    print(f"âœ… Fixed node {node.get('id', 'unknown')} evidence {i+1}")
    
    # ã‚¨ãƒƒã‚¸ã®è¨¼æ‹ ã‚’ä¿®æ­£
    for edge in graph_data.get('edges', []):
        evidences = edge.get('evidence', [])
        for i, evidence_item in enumerate(evidences):
            if isinstance(evidence_item, dict):
                evidence_text = evidence_item.get('text', '')
                if evidence_text in manual_fixes:
                    evidence_item['text'] = manual_fixes[evidence_text]
                    fixes_made += 1
                    print(f"âœ… Fixed edge {edge.get('source', 'unknown')} -> {edge.get('target', 'unknown')} evidence {i+1}")
    
    # ä¿®æ­£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    if fixes_made > 0:
        save_graph_data(graph_file, graph_data)
        print(f"ğŸ’¾ Saved {fixes_made} fixes to {graph_file.name}")
    
    return fixes_made


def main():
    graph_dir = Path('webui/public')
    source_dir = Path('extra-input')
    
    graph_files = [
        graph_dir / 'graph_extra-1.json',
        graph_dir / 'graph_extra-2.json',
        graph_dir / 'graph_extra-3.json'
    ]
    
    total_fixes = 0
    
    for graph_file in graph_files:
        if not graph_file.exists():
            print(f"âš ï¸  Graph file not found: {graph_file}")
            continue
            
        print(f"ğŸ”§ Fixing {graph_file.name}...")
        fixes = fix_evidence_in_graph(graph_file, source_dir)
        total_fixes += fixes
    
    print(f"\nâœ¨ Total fixes applied: {total_fixes}")
    print("ğŸ” Run 'python validate_evidence.py' to verify the fixes")
    
    return 0


if __name__ == '__main__':
    exit(main())