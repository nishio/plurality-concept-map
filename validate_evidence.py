#!/usr/bin/env python3
"""
è¨¼æ‹ ãƒ†ã‚­ã‚¹ãƒˆã¨åŸæ–‡ã®æ•´åˆæ€§ã‚’æ¤œè¨¼ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Text fragment linkãŒå¤±æ•—ã™ã‚‹åŸå› ã¨ãªã‚‹ä¸æ­£ç¢ºãªå¼•ç”¨ã‚’æ¤œå‡º
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import argparse
from difflib import SequenceMatcher


def load_graph_data(graph_file: Path) -> Dict:
    """ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    with open(graph_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_source_text(source_file: Path) -> str:
    """ã‚½ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    with open(source_file, 'r', encoding='utf-8') as f:
        content = f.read()
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€å†’é ­ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»
        lines = content.split('\n')
        # URLã‚„ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æœ¬æ–‡ã‚’å–å¾—
        content_start = 0
        for i, line in enumerate(lines):
            if line.strip() and not line.startswith('http') and '|' not in line and not line.startswith('#'):
                # æœ¬æ–‡ã®é–‹å§‹ã‚’æ¤œå‡º
                content_start = i
                break
        return '\n'.join(lines[content_start:])


def normalize_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ï¼ˆç©ºç™½ã€æ”¹è¡Œã€å¥èª­ç‚¹ã®çµ±ä¸€ï¼‰"""
    # æ”¹è¡Œã‚’ç©ºç™½ã«å¤‰æ›
    text = re.sub(r'\s+', ' ', text)
    # å¥èª­ç‚¹å‰å¾Œã®ç©ºç™½ã‚’èª¿æ•´
    text = re.sub(r'\s*([ã€‚ã€ï¼ï¼Ÿ])\s*', r'\1', text)
    # å…¨è§’ãƒ»åŠè§’ã®çµ±ä¸€
    text = text.replace('ã€€', ' ')  # å…¨è§’ç©ºç™½ã‚’åŠè§’ã«
    return text.strip()


def find_text_in_source(evidence_text: str, source_text: str, threshold: float = 0.8) -> Optional[Tuple[str, float, int, int]]:
    """
    ã‚½ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆå†…ã§è¨¼æ‹ ãƒ†ã‚­ã‚¹ãƒˆã«æœ€ã‚‚è¿‘ã„éƒ¨åˆ†ã‚’æ¤œç´¢
    
    Returns:
        (matched_text, similarity, start_pos, end_pos) ã¾ãŸã¯ None
    """
    evidence_normalized = normalize_text(evidence_text)
    source_normalized = normalize_text(source_text)
    
    # ã¾ãšå®Œå…¨ä¸€è‡´ã‚’è©¦è¡Œ
    if evidence_normalized in source_normalized:
        start_pos = source_normalized.find(evidence_normalized)
        end_pos = start_pos + len(evidence_normalized)
        return evidence_normalized, 1.0, start_pos, end_pos
    
    # éƒ¨åˆ†ä¸€è‡´ã‚’æ¤œç´¢ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰
    evidence_length = len(evidence_normalized)
    best_match = None
    best_similarity = 0.0
    
    # å‰å¾Œã«ä½™è£•ã‚’æŒãŸã›ãŸæ¤œç´¢ç¯„å›²
    search_ranges = [
        evidence_length,  # åŒã˜é•·ã•
        int(evidence_length * 1.2),  # 20%é•·ã„
        int(evidence_length * 0.8),   # 20%çŸ­ã„
    ]
    
    for window_size in search_ranges:
        for i in range(len(source_normalized) - window_size + 1):
            window_text = source_normalized[i:i + window_size]
            similarity = SequenceMatcher(None, evidence_normalized, window_text).ratio()
            
            if similarity > best_similarity and similarity >= threshold:
                best_similarity = similarity
                best_match = (window_text, similarity, i, i + window_size)
    
    return best_match


def validate_evidence_in_graph(graph_file: Path, source_dir: Path) -> List[Dict]:
    """ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å…¨è¨¼æ‹ ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œè¨¼"""
    graph_data = load_graph_data(graph_file)
    issues = []
    
    # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
    if 'extra-1' in str(graph_file):
        source_file = source_dir / '1.md'
    elif 'extra-2' in str(graph_file):
        source_file = source_dir / '2.md'
    elif 'extra-3' in str(graph_file):
        source_file = source_dir / '3.md'
    else:
        print(f"Unknown graph file pattern: {graph_file}")
        return issues
    
    if not source_file.exists():
        issues.append({
            'type': 'missing_source',
            'message': f"Source file not found: {source_file}",
            'graph_file': str(graph_file)
        })
        return issues
    
    source_text = load_source_text(source_file)
    
    # ãƒãƒ¼ãƒ‰ã®è¨¼æ‹ ã‚’æ¤œè¨¼
    for node in graph_data.get('nodes', []):
        node_id = node.get('id', 'unknown')
        evidences = node.get('evidence', [])
        
        for i, evidence_item in enumerate(evidences):
            # evidenceãŒè¾æ›¸å½¢å¼ã®å ´åˆã¯textãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
            if isinstance(evidence_item, dict):
                evidence_text = evidence_item.get('text', '')
            else:
                evidence_text = str(evidence_item)
                
            if not evidence_text.strip():
                continue
                
            match_result = find_text_in_source(evidence_text, source_text)
            
            if match_result is None:
                issues.append({
                    'type': 'evidence_not_found',
                    'node_id': node_id,
                    'evidence_index': i,
                    'evidence_text': evidence_text[:100] + ('...' if len(evidence_text) > 100 else ''),
                    'graph_file': str(graph_file),
                    'source_file': str(source_file)
                })
            elif match_result[1] < 1.0:  # å®Œå…¨ä¸€è‡´ã§ãªã„å ´åˆ
                matched_text, similarity, start_pos, end_pos = match_result
                issues.append({
                    'type': 'evidence_mismatch',
                    'node_id': node_id,
                    'evidence_index': i,
                    'evidence_text': evidence_text,
                    'matched_text': matched_text,
                    'similarity': similarity,
                    'graph_file': str(graph_file),
                    'source_file': str(source_file)
                })
    
    # ã‚¨ãƒƒã‚¸ã®è¨¼æ‹ ã‚’æ¤œè¨¼
    for edge in graph_data.get('edges', []):
        source_id = edge.get('source', 'unknown')
        target_id = edge.get('target', 'unknown')
        evidences = edge.get('evidence', [])
        
        for i, evidence_item in enumerate(evidences):
            # evidenceãŒè¾æ›¸å½¢å¼ã®å ´åˆã¯textãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
            if isinstance(evidence_item, dict):
                evidence_text = evidence_item.get('text', '')
            else:
                evidence_text = str(evidence_item)
                
            if not evidence_text.strip():
                continue
                
            match_result = find_text_in_source(evidence_text, source_text)
            
            if match_result is None:
                issues.append({
                    'type': 'evidence_not_found',
                    'edge': f"{source_id} -> {target_id}",
                    'evidence_index': i,
                    'evidence_text': evidence_text[:100] + ('...' if len(evidence_text) > 100 else ''),
                    'graph_file': str(graph_file),
                    'source_file': str(source_file)
                })
            elif match_result[1] < 1.0:  # å®Œå…¨ä¸€è‡´ã§ãªã„å ´åˆ
                matched_text, similarity, start_pos, end_pos = match_result
                issues.append({
                    'type': 'evidence_mismatch',
                    'edge': f"{source_id} -> {target_id}",
                    'evidence_index': i,
                    'evidence_text': evidence_text,
                    'matched_text': matched_text,
                    'similarity': similarity,
                    'graph_file': str(graph_file),
                    'source_file': str(source_file)
                })
    
    return issues


def print_validation_report(issues: List[Dict]):
    """æ¤œè¨¼çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ã§å‡ºåŠ›"""
    if not issues:
        print("âœ… All evidence texts match their source files perfectly!")
        return
    
    print(f"âš ï¸  Found {len(issues)} evidence validation issues:\n")
    
    # å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥ã«æ•´ç†
    by_type = {}
    for issue in issues:
        issue_type = issue['type']
        if issue_type not in by_type:
            by_type[issue_type] = []
        by_type[issue_type].append(issue)
    
    for issue_type, type_issues in by_type.items():
        print(f"## {issue_type.upper().replace('_', ' ')} ({len(type_issues)} issues)")
        
        for issue in type_issues:
            graph_file = Path(issue['graph_file']).name
            
            if 'node_id' in issue:
                print(f"ğŸ“ Node: {issue['node_id']} (in {graph_file})")
            elif 'edge' in issue:
                print(f"ğŸ“ Edge: {issue['edge']} (in {graph_file})")
            
            if issue_type == 'evidence_mismatch':
                print(f"   Similarity: {issue['similarity']:.2f}")
                print(f"   ğŸ“ Evidence: {issue['evidence_text'][:150]}...")
                print(f"   ğŸ“„ Source:   {issue['matched_text'][:150]}...")
            elif issue_type == 'evidence_not_found':
                print(f"   ğŸ“ Evidence: {issue['evidence_text']}")
            
            print(f"   ğŸ“‚ Source: {Path(issue['source_file']).name}")
            print()
        
        print()


def main():
    parser = argparse.ArgumentParser(description='Validate evidence texts against source files')
    parser.add_argument('--graph-dir', type=str, default='webui/public', 
                       help='Directory containing graph JSON files')
    parser.add_argument('--source-dir', type=str, default='extra-input',
                       help='Directory containing source markdown files')
    parser.add_argument('--threshold', type=float, default=0.8,
                       help='Similarity threshold for matches (0.0-1.0)')
    
    args = parser.parse_args()
    
    graph_dir = Path(args.graph_dir)
    source_dir = Path(args.source_dir)
    
    if not graph_dir.exists():
        print(f"âŒ Graph directory not found: {graph_dir}")
        return 1
    
    if not source_dir.exists():
        print(f"âŒ Source directory not found: {source_dir}")
        return 1
    
    # extraãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å¯¾è±¡ã«æ¤œè¨¼
    graph_files = [
        graph_dir / 'graph_extra-1.json',
        graph_dir / 'graph_extra-2.json', 
        graph_dir / 'graph_extra-3.json'
    ]
    
    all_issues = []
    
    for graph_file in graph_files:
        if not graph_file.exists():
            print(f"âš ï¸  Graph file not found: {graph_file}")
            continue
            
        print(f"ğŸ” Validating {graph_file.name}...")
        issues = validate_evidence_in_graph(graph_file, source_dir)
        all_issues.extend(issues)
    
    print(f"\n{'='*60}")
    print("VALIDATION REPORT")
    print('='*60)
    print_validation_report(all_issues)
    
    return 0 if not all_issues else 1


if __name__ == '__main__':
    exit(main())